# LLM and Society
Recent papers on (1) Psychology of LLMs; (2) Biases in LLMs; (3) Jailbreaking LLMs.

## Trait Theory
- <a href="https://arxiv.org/abs/2206.07550">[Paper]</a> Evaluating and Inducing Personality in Pre-trained Language Models, Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu, *June 2022*
- <a href="https://arxiv.org/abs/2209.14338">[Paper]</a> Who is GPT-3? An Exploration of Personality, Values and Demographics, Marilù Miotto, Nicola Rossberg, Bennett Kleinberg, *September 2022*
- <a href="https://arxiv.org/abs/2204.12000">[Paper]</a> Estimating the Personality of White-Box Language Models, Saketh Reddy Karra, Son The Nguyen, Theja Tulabandhula, *April 2022*
- <a href="https://arxiv.org/abs/2304.07333">[Paper]</a> The Self-Perception and Political Biases of ChatGPT, Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Markus Pauly, *April 2023*
- <a href="https://www.researchsquare.com/article/rs-2717108/v1">[Paper]</a> Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics, Peter Romero, Stephen Fitz, Teruo Nakatsuma, *May 2023*
- <a href="https://arxiv.org/abs/2305.14693">[Paper]</a>Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs, Xiaoyang Song, Akshat Gupta, Kiyan Mohebbizadeh, Shujie Hu, Anant Singh, *May 2023*
- <a href="https://arxiv.org/abs/2305.19926v2">[Paper]</a> ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models, Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R. Lyu, *May 2023*
- <a href="https://arxiv.org/abs/2306.04308">[Paper]</a> Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results, Bojana Bodroza, Bojana M. Dinic, Ljubisa Bojic, *June 2023*
- <a href="https://arxiv.org/abs/2307.00184">[Paper]</a> Personality Traits in Large Language Models, Mustafa Safdari, Greg Serapio-García, Clément Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, Maja Matarić, *July 2023*
- <a href="https://arxiv.org/abs/2307.16180">[Paper]</a> Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models, Keyu Pan, Yawen Zeng, *July 2023*

## Other Psychological Topics
- <a href="https://arxiv.org/abs/2212.10529">[Paper]</a> Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective, Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, Lidong Bing, *December 2022*
- <a href="https://arxiv.org/abs/2304.11111">[Paper]</a> Inducing anxiety in large language models increases exploration and bias, Julian Coda-Forno, Kristin Witte, Akshay K. Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz, *April 2023*
- <a href="https://arxiv.org/abs/2305.12564">[Paper]</a> ChatGPT Is More Likely to Be Perceived as Male Than Female, Jared Wong, Jin Kim, *May 2023*
- <a href="https://arxiv.org/abs/2307.09042">[Paper]</a> Emotional Intelligence of Large Language Models, Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia, *July 2023*
- <a href="https://arxiv.org/abs/2307.11760">[Paper]</a> EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus, Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang, Wenxin Hou, Jianxun Lian, Xing Xie, *July 2023*
- <a href="https://arxiv.org/abs/2307.13779">[Paper]</a> Is GPT a Computational Model of Emotion? Detailed Analysis, Ala N. Tak, Jonathan Gratch, *July 2023*
- <a href="https://arxiv.org/abs/2307.14324">[Paper]</a> Evaluating the Moral Beliefs Encoded in LLMs, Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei, *July 2023*
- <a href="https://arxiv.org/abs/2308.01264">[Paper]</a> Exploring the Psychology of GPT-4's Moral and Legal Reasoning, Guilherme F. C. F. Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, Marcelo de Araújo, *August 2023*
- <a href="https://arxiv.org/abs/2308.01834">[Paper]</a> The Capability of Large Language Models to Measure Psychiatric Functioning, Isaac R. Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, Matteo Malgaroli, *August 2023*
- <a href="https://arxiv.org/abs/2308.03527">[Paper]</a> Exploring ChatGPT's Empathic Abilities, Kristina Schaaff, Caroline Reinig, Tim Schlippe, *August 2023*
- <a href="https://arxiv.org/abs/2308.03656">[Paper]</a> Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench, Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu, *August 2023*

## Stereotype and Bias
- <a href="https://arxiv.org/abs/2303.01248">[Paper]</a> Can ChatGPT Assess Human Personalities? A General Evaluation Framework, Haocong Rao, Cyril Leung, Chunyan Miao, *March 2023*
- <a href="https://arxiv.org/abs/2304.03738">[Paper]</a> Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models, Emilio Ferrara, *April 2023*
- <a href="https://arxiv.org/abs/2305.02547">[Paper]</a> PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences, Hang Jiang, Xiajie Zhang, Xubo Cao, Jad Kabbara, *May 2023*
- <a href="https://arxiv.org/abs/2305.08283">[Paper]</a> From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models, Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov, *May 2023*
- <a href="https://arxiv.org/abs/2305.10510">[Paper]</a> ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages, Sourojit Ghosh, Aylin Caliskan, *May 2023*
- <a href="https://arxiv.org/abs/2305.14610">[Paper]</a> This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models, Bryan Li, Chris Callison-Burch, *May 2023*
- <a href="https://arxiv.org/abs/2305.17926">[Paper]</a> Large Language Models are not Fair Evaluators, Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, *May 2023*
- <a href="https://arxiv.org/abs/2305.18189">[Paper]</a> Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models, Myra Cheng, Esin Durmus, Dan Jurafsky, *May 2023*
- <a href="https://psyarxiv.com/27u6z/">[Paper]</a> Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students, Katherine Elizabeth Abramski, Salvatore Citraro, Luigi Lombardi, Giulio Rossetti, Massimo Stella, *May 2023*
- <a href="https://arxiv.org/abs/2306.02294">[Paper]</a> Exposing Bias in Online Communities through Large-Scale Language Models, Celine Wald, Lukas Pfahler, *June 2023*
- <a href="https://arxiv.org/abs/2306.02428">[Paper]</a> Taught by the Internet, Exploring Bias in OpenAIs GPT3, Ali Ayaz, Aditya Nawalgaria, Ruilian Yin, *June 2023*
- <a href="https://arxiv.org/abs/2306.04735">[Paper]</a> Soft-prompt Tuning for Large Language Models to Evaluate Bias, Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval Pandya, Laleh Seyyed-Kalantari, Faiza Khan Khattak, *June 2023*

## Cross-Cultural Difference
- <a href="https://arxiv.org/abs/2301.01768">[Paper]</a> The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation, Jochen Hartmann, Jasper Schwenzow, Maximilian Witte, *January 2023*
- <a href="https://arxiv.org/abs/2203.13722">[Paper]</a> Probing Pre-Trained Language Models for Cross-Cultural Differences in Values, Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein, *March 2023*
- <a href="https://arxiv.org/abs/2303.17466">[Paper]</a> Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study, Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, Daniel Hershcovich, *March 2023*
- <a href="https://arxiv.org/abs/2303.17548">[Paper]</a> Whose Opinions Do Language Models Reflect, Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, *March 2023*
- <a href="https://arxiv.org/abs/2305.14456">[Paper]</a> Having Beer after Prayer? Measuring Cultural Bias in Large Language Models, Tarek Naous, Michael J. Ryan, Wei Xu, *May 2023*
- <a href="https://arxiv.org/abs/2305.16339">[Paper]</a> Don't Trust GPT When Your Question Is Not In English, Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak, *May 2023*
- <a href="https://arxiv.org/abs/2306.00639">[Paper]</a> Being Right for Whose Right Reasons?, Terne Sasha Thorn Jakobsen, Laura Cabello, Anders Søgaard, *June 2023*
- <a href="https://arxiv.org/abs/2306.16388">[Paper]</a> Towards Measuring the Representation of Subjective Global Opinions in Language Models, Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, Deep Ganguli, *June 2023*
- <a href="https://arxiv.org/abs/2307.14324">[Paper]</a> Evaluating the Moral Beliefs Encoded in LLMs, Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei, *July 2023*

## Toxicity and Jailbreaking
- <a href="https://arxiv.org/abs/2211.09527">[Paper]</a> Ignore Previous Prompt: Attack Techniques For Language Models, Fábio Perez, Ian Ribeiro, *November 2022*
- <a href="https://openreview.net/forum?id=89qDzjrWHLs">[Paper]</a> Can Large Language Models Truly Follow your Instructions? Joel Jang, Seongheyon Ye, Minjoon Seo, *December 2022*
- <a href="https://arxiv.org/abs/2301.12867">[Paper]</a> Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity, Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing, *January 2023*
- <a href="https://arxiv.org/abs/2304.05335">[Paper]</a> Toxicity in ChatGPT: Analyzing Persona-assigned Language Models, Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, *April 2023*
- <a href="https://arxiv.org/abs/2304.12244">[Paper]</a> WizardLM: Empowering Large Language Models to Follow Complex Instructions, Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, *April 2023*
- <a href="https://www.researchsquare.com/article/rs-2873090/v1">[Paper]</a> Defending ChatGPT against Jailbreak Attack via Self-Reminder, Fangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, *June 2023*
- <a href="https://arxiv.org/abs/2306.04528">[Paper]</a> PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts, Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zeek Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie, *June 2023*
- <a href="https://arxiv.org/abs/2306.05499">[Paper]</a> Prompt Injection attack against LLM-integrated Applications, Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu, *June 2023*
- <a href="https://arxiv.org/abs/2306.09255">[Paper]</a> Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations, Attia Qammar, Hongmei Wang, Jianguo Ding, Abdenacer Naouri, Mahmoud Daneshmand, Huansheng Ning, *June 2023*
- <a href="https://arxiv.org/abs/2306.15447">[Paper]</a> Are aligned neural networks adversarially aligned?, Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt, *June 2023*
- <a href="https://arxiv.org/abs/2306.16244">[Paper]</a> CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models, Yufei Huang, Deyi Xiong, *June 2023*
- <a href="https://arxiv.org/abs/2307.02483">[Paper]</a> Jailbroken: How Does LLM Safety Training Fail?, Alexander Wei, Nika Haghtalab, Jacob Steinhardt, *July 2023*
